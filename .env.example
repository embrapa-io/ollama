PORT=11434
LLM_PATH=/mnt/llm

# Ollama Performance Optimizations
# ---------------------------------
# Context window size (default: 2048)
OLLAMA_CONTEXT_LENGTH=16384

# Flash Attention - faster inference, lower VRAM usage
OLLAMA_FLASH_ATTENTION=1

# New inference engine (improved performance)
OLLAMA_NEW_ENGINE=1

# Multi-GPU Configuration (dual RTX 6000 = 48GB total)
# Comma-separated GPU IDs for layer distribution
CUDA_VISIBLE_DEVICES=0,1
NVIDIA_VISIBLE_DEVICES=all

# Concurrent Requests (adjust based on model size and VRAM)
# - For 70B models: 2-4 parallel requests
# - For smaller models (7B-13B): 4-8 parallel requests
OLLAMA_NUM_PARALLEL=4

# Keep models in VRAM (reduces cold start latency)
# Use 24h for production, -1 for indefinite
OLLAMA_KEEP_ALIVE=24h
